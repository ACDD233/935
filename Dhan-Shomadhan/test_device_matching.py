#!/usr/bin/env python3
"""
æµ‹è¯•è®¾å¤‡åŒ¹é…ä¿®å¤
"""
import torch
import numpy as np
from ultralytics import YOLO

def test_device_matching():
    """æµ‹è¯•è®¾å¤‡åŒ¹é…ä¿®å¤"""
    print("=" * 60)
    print("æµ‹è¯•è®¾å¤‡åŒ¹é…ä¿®å¤")
    print("=" * 60)
    
    # 1. æµ‹è¯•æ¨¡å‹åŠ è½½å’Œè®¾å¤‡è®¾ç½®
    model_path = "Dhan-Shomadhan/1343258/models/fold_1_best.pt"
    try:
        yolo_model = YOLO(model_path)
        torch_model = yolo_model.model
        torch_model.eval()
        
        # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        torch_model = torch_model.to(device)
        
        model_device = next(torch_model.parameters()).device
        print(f"âœ… æ¨¡å‹è®¾å¤‡: {model_device}")
        print(f"âœ… ç›®æ ‡è®¾å¤‡: {device}")
        print(f"âœ… è®¾å¤‡åŒ¹é…: {model_device == torch.device(device)}")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return False
    
    # 2. æµ‹è¯•æ¨¡å‹åŒ…è£…å™¨
    class ModelWrapper(torch.nn.Module):
        def __init__(self, model):
            super(ModelWrapper, self).__init__()
            self.model = model
            self.model.train()
            # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            self.device = next(model.parameters()).device
            self.model = self.model.to(self.device)
            print(f"âœ… åŒ…è£…å™¨è®¾å¤‡: {self.device}")
        
        def forward(self, x):
            # ç¡®ä¿è¾“å…¥åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            if x.device != self.device:
                x = x.to(self.device)
                print(f"âœ… è¾“å…¥è®¾å¤‡è°ƒæ•´: {x.device}")
            
            if not x.requires_grad:
                x = x.requires_grad_(True)
            
            with torch.enable_grad():
                output = self.model(x)
            
            if isinstance(output, tuple):
                return output[0]
            return output
    
    try:
        wrapped_model = ModelWrapper(torch_model)
        print("âœ… æ¨¡å‹åŒ…è£…å™¨åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŒ…è£…å™¨åˆ›å»ºå¤±è´¥: {e}")
        return False
    
    # 3. æµ‹è¯•è¾“å…¥å¼ é‡
    try:
        test_input = torch.randn(1, 3, 320, 320).to(device)
        print(f"âœ… è¾“å…¥å¼ é‡è®¾å¤‡: {test_input.device}")
        print(f"âœ… è¾“å…¥å¼ é‡ç±»å‹: {test_input.dtype}")
        print(f"âœ… è¾“å…¥å¼ é‡å½¢çŠ¶: {test_input.shape}")
        
        # å¯ç”¨æ¢¯åº¦
        test_input.requires_grad_(True)
        print(f"âœ… è¾“å…¥å¼ é‡æ¢¯åº¦: {test_input.requires_grad}")
        
    except Exception as e:
        print(f"âŒ è¾“å…¥å¼ é‡åˆ›å»ºå¤±è´¥: {e}")
        return False
    
    # 4. æµ‹è¯•å‰å‘ä¼ æ’­
    try:
        with torch.enable_grad():
            output = wrapped_model(test_input)
            print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸ")
            print(f"âœ… è¾“å‡ºç±»å‹: {type(output)}")
            if hasattr(output, 'shape'):
                print(f"âœ… è¾“å‡ºå½¢çŠ¶: {output.shape}")
            if hasattr(output, 'device'):
                print(f"âœ… è¾“å‡ºè®¾å¤‡: {output.device}")
                
    except Exception as e:
        print(f"âŒ å‰å‘ä¼ æ’­å¤±è´¥: {e}")
        return False
    
    # 5. æµ‹è¯• pytorch-grad-cam å¯¼å…¥
    try:
        from pytorch_grad_cam import GradCAM
        print("âœ… pytorch-grad-cam å¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"âŒ pytorch-grad-cam å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    print("\nğŸ‰ è®¾å¤‡åŒ¹é…æµ‹è¯•é€šè¿‡!")
    return True

if __name__ == '__main__':
    test_device_matching()
